{% extends "base.html" %}
{% block content %}
<h1>AdaBoost</h1>
    <h3>Overview</h3>
    <p>Ensemble learning is a supervised machine learning approach in which multiple models are combined to make a single,
        stronger predictor. The core idea is that even if individual models are imperfect, their errors often differ, so
        pooling their predictions can produce a result that is more accurate and stable than any one model alone. Boosting
        is a specific kind of ensemble method where models are trained sequentially, not in parallel. It starts with a weak
        learner - a simple model that does only a bit better than guessing. On its own, a weak learner isn’t very accurate,
        but boosting turns many weak learners into a strong model by letting each new learner focus on the mistakes made so
        far. In AdaBoost, this happens through re-weighting: after each round, the samples that were misclassified get higher
        weights, so the next weak learner pays extra attention to them. Over many rounds, AdaBoost combines all weak learners
        into one final predictor, giving more influence to learners that performed well, therefore steadily increasing overall
        accuracy.</p>

    <figure>
        <img src="{{ url_for('static', path='figures/weak_v_strong_learners.jpg') }}" alt="Weak v Strong Learners" width="600" style="display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center;">
            Fig. 1
        </figcaption>
        <figcaption style="text-align: center;">
            Sequential weak learners are trained on increasingly weighted mistakes, and their combined decision boundaries form a stronger final classifier.
        </figcaption>
    </figure>

    <p>Although AdaBoost, Gradient Boosting, and XGBoost all belong to the boosting family, they differ in how they decide
        what to fix next. AdaBoost adapts by weighting the training examples, pushing later learners toward the hardest cases.
        Gradient Boosting instead builds each new learner to predict the residuals of the current model, so each round directly
        reduces what the ensemble is still getting wrong. XGBoost is an advanced, highly optimized form of gradient boosting
        that adds strong regularization, efficient handling of missing data, and more sophisticated loss minimization, which
        often makes it faster and more accurate in practice, though it can be more complex to tune. Across all these methods,
        boosting works by iteratively improving the model: early learners reduce large systematic mistakes (lowering bias),
        while later learners refine smaller errors and stabilize predictions (reducing variance). In AdaBoost this looks like
        shifting attention toward misclassified days, while in gradient-based methods it looks like shrinking residual errors
        round after round.</p>

    <p>The behavior of boosting models depends heavily on a few core hyperparameters. The learning rate controls how much each
        weak learner contributes; smaller values mean slower, steadier improvements that usually generalize better, while larger
        values learn faster but risk overfitting. The number of estimators sets how many weak learners are added, which can
        increase accuracy as long as learning rate and model complexity stay reasonable. Finally, tree depth determines how
        complex each weak learner is: AdaBoost usually works best with very shallow trees (depth 1–3), keeping each learner
        weak enough to avoid overfitting while still allowing the full ensemble to capture complex patterns. For this project,
        this matters because AdaBoost can gradually learn the difference between predictable crowd days and tricky edge
        cases, like partial ride shutdowns or unusual weather, by repeatedly targeting the hardest-to-classify days and
        combining those improvements into a single strong crowd-level predictor.</p>

    <figure>
        <img src="{{ url_for('static', path='figures/AdaBoosting.png') }}" alt="AdaBoosting" width="600" style="display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center;">
            Fig. 2
        </figcaption>
        <figcaption style="text-align: center;">
            An example of an AdaBoost workflow. Each classifier is trained on re-weighted data that emphasizes previous mistakes, and their combined outputs form a final ensemble with improved accuracy.
        </figcaption>
    </figure>

    <h3>Data Prep</h3>
    <p>For this boosting module, I used my cleaned ride wait-time dataset, which contains real-time snapshots of Disney ride
        waits collected throughout the semester. This dataset was selected because it is large, frequently updated, and
        directly reflects guest experiences in the parks through measurable wait-time behavior. </p>

    <figure>
      <img src="{{ url_for('static', path='figures/adaboost_labeled_dataset.png') }}"
           alt="AdaBoost labeled" width="600"
           style="display: block; margin-left: auto; margin-right: auto;">
      <figcaption style="text-align: center;">Fig. 3</figcaption>
        <figcaption style="text-align: center;">
                Sample of labeled dataset ready for boosting.
            </figcaption>
    </figure>

    <p><a href="https://github.com/A-Derks/Machine_Learning_Case_Study/blob/main/data/clean/adaboost_day_full.csv">Here</a> is a link to the full prepped dataset to be used for boosting.</p>

    <p>To apply supervised boosting, I first needed a clear target variable (label). Since crowd labels are not provided
        directly in the Queue-Times data, I created them from the wait-time distribution itself. After aggregating ride-level
        waits into daily summaries, I defined a day as “busy” if its 95th percentile wait time was above the median of all days,
        and “not_busy” otherwise. This produces a balanced label split and gives AdaBoost a meaningful outcome to learn from:
        predicting whether a day reflects high-crowd conditions based only on wait-time patterns.</p>

    <p>Once the target variable was defined, I split the labeled dataset into a training set (80%) and a testing set (20%)
        using a stratified train_test_split. The training set is the only portion used to build the AdaBoost model, while the
        testing set is held aside to evaluate accuracy on unseen days. These sets must be disjoint (no overlapping rows)
        so that performance reflects real predictive ability rather than memorization; if the same day appeared in both sets,
        the model’s accuracy would be artificially inflated and not meaningful for real-world forecasting.</p>

    <figure>
      <img src="{{ url_for('static', path='figures/adaboost_train_test_split.png') }}"
           alt="AdaBoost train test split" width="600"
           style="display: block; margin-left: auto; margin-right: auto;">
      <figcaption style="text-align: center;">Fig. 4</figcaption>
        <figcaption style="text-align: center;">
                Train/ test split.
            </figcaption>
    </figure>

    <p>Finally, I prepared the features to be fully suitable for boosting. AdaBoost in scikit-learn requires numeric inputs,
        so I used only numeric daily features: average wait, median wait, 95th percentile wait, and percent of rides open.
        Any missing values in these numeric columns were handled with median imputation fit only on the training data,
        ensuring the model never “peeks” at test information. Because I did not include categorical predictors in this
        version (like ride type or day-of-week), no one-hot encoding was needed — the final processed feature matrices
        (X_train_proc and X_test_proc) are purely numerical and ready for boosting.</p>

    <h3>Code</h3>

    <p><a href="https://github.com/A-Derks/Machine_Learning_Case_Study/blob/main/adaboost.py">Here</a> is a link to the code used to prep the data and perform the AdaBoost.</p>

    <h3>Results</h3>
    <p>Using the labeled daily dataset (49 total days), AdaBoost was trained to classify each day as “busy” or “not_busy”
      using day-level numeric features: average wait time, median wait time, and the percent of rides open. Labels were
      defined from queue-times alone: a day was marked busy when p95_wait ≥ 40 minutes, producing a reasonably balanced
      class distribution (32 busy vs 17 not_busy). The data was split into 39 training days and 10 testing days with
      stratification to preserve class balance, so the testing set represented unseen days.</p>

    <p>The parameter sweep tested multiple tree depths, numbers of estimators, and learning rates. The best configuration was:</p>

    <ul>
      <li><strong>Base tree depth</strong> : 2 (weak learner)</li>
      <li><strong>n_estimators</strong> : 10</li>
      <li><strong>learning_rate</strong> : 1.0</li>
      <li><strong>Test accuracy</strong> : 0.90</li>
    </ul>

    <p>This means the model correctly classified 9 out of 10 test days.</p>

    <p>To understand how boosting performance depends on model settings, a parameter sweep was run across different weak-learner
      tree depths (1, 2, and 3), numbers of estimators (10–200), and learning rates (0.05, 0.1, 0.5, 1.0). The accuracy-versus-estimators
      plots show that depth matters more than simply adding estimators: depth = 1 stays near a steady accuracy plateau, and depth = 3
      also remains flat at a lower level, meaning adding more estimators does not improve performance once weak learners hit their limit.
      Depth = 2 is where AdaBoost becomes sensitive to tuning — accuracy varies across estimator counts and learning rates, and the best
      performance occurs early with a small ensemble (n_estimators = 10) and a higher learning rate, reaching 0.90 accuracy. This suggests
      that a compact boosted model with medium-strength learners captures the crowd signal most effectively without needing a large stack
      of estimators.</p>

    <p>The confusion matrix confirms where the model succeeds and where it struggles. All 7 busy days in the test set were predicted correctly,
      but 1 of the 3 not_busy days was mislabeled as busy. That produces one false positive for the busy class and one false negative for not_busy,
      which is why busy recall is perfect (1.00) while not_busy recall is lower (0.67). Overall, AdaBoost still separates crowd-heavy and crowd-light
      days very well, with only a single borderline day falling on the wrong side of the decision boundary.</p>

    <figure>
      <img src="{{ url_for('static', path='figures/adaboost_confusion_matrix.png') }}"
           alt="AdaBoost confusion matrix" width="600"
           style="display: block; margin-left: auto; margin-right: auto;">
      <figcaption style="text-align: center;">Fig. 8</figcaption>
      <figcaption style="text-align: center;">AdaBoost confusion matrix.</figcaption>
    </figure>

    <p>The feature-importance plot helps explain why the model performed strongly: percent of rides open (pct_open) was the dominant predictor,
      with average wait time (avg_wait) providing secondary support, and median wait time (med_wait) contributing only slightly. Together, these
      results show that AdaBoost can distinguish calm versus crowded Disney days effectively from simple daily wait-time and operational summaries.</p>

    <figure>
      <img src="{{ url_for('static', path='figures/adaboost_feature_importance.png') }}"
           alt="AdaBoost feature importance" width="600"
           style="display: block; margin-left: auto; margin-right: auto;">
      <figcaption style="text-align: center;">Fig. 9</figcaption>
      <figcaption style="text-align: center;">AdaBoost feature importance.</figcaption>
    </figure>


    <h3>Conclusions</h3>
    <p>The AdaBoost analysis shows that Disney park busyness can be predicted reliably from daily wait-time behavior and park
      operations. By combining multiple weak learners into a single ensemble, boosting forms a strong classifier that captures
      the boundary between crowded and calm days. On unseen test data, the model achieved 0.90 accuracy, correctly identifying
      every busy day and missing only one not_busy day, which reinforces that these daily features contain a stable signal tied
      to real guest experience.</p>

    <p>These results also highlight the importance of hyperparameters in boosting. The strongest model used medium-depth weak
      learners (depth = 2) and a small number of estimators, showing that AdaBoost does not need a large or highly complex ensemble
      to perform well here. The sweep plots suggest that performance peaks quickly and then levels off, so pushing to deeper trees
      or many more estimators would likely add complexity without consistent gains and could increase overfitting risk as the dataset grows.</p>

    <p>In terms of the project topic, the model suggests that operational conditions and crowd intensity move together in predictable
      ways: when fewer rides are available and overall waits rise, the day reliably behaves like a busy crowd day. Future improvements
      could come from extending the dataset across more months and seasons, then re-testing generalization under holidays, extreme
      weather, or unusual closures. Another meaningful extension would be shifting from binary classification to regression to predict
      the magnitude of peak waits, allowing the model to estimate not just whether a day is busy, but how busy it will be.</p>




{% endblock %}