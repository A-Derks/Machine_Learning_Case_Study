{% extends "base.html" %}
{% block content %}
<h1>Clustering</h1>
    <h3>Overview</h3>
    <p>Clustering is an unsupervised machine learning technique used to group similar data points together
    based on their features. The goal is to discover natural patterns in the data, such that objects within
    the same group are more alike to each other than to those in different groups. There are two main clusering
    approaches: partitional clustering and hierarchical clustering. Partitional methods, such as k-means, split
    the dataset into a specified number of clusters by iteratively assigning points to the nearest cluster center
    and updating those centers (See Fig. 1 below). Hierarchical clustering builds a nested tree of clusters, either by starting with
    each data point as its own cluster and merging them step by step, or by starting with all points in
    one cluster and splitting them recursively (See Fig 2 below). Both partitional and hierarchical clustering rely on distance
    metrics to measure similarity between data points. Common metrics include Euclidean distance (straight-line
    distance), Manhattan distance (sum of absolute differences), and cosine similarity (angle between vectors).
    The choice of clustering method and distance metric has a strong influence on the shapes and types of clusters
    that are discovered.<p>

    <figure>
        <img src="{{ url_for('static', path='figures/k_means_clustering_befire_after.png') }}" alt="Clustering data" width="600" style="display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center;">
            Fig. 1
        </figcaption>
        <figcaption style="text-align: center;">
            An illustration of k-means clustering. On the left, data points are scattered without structure. On the right,
            the algorithm has grouped them into distinct clusters around their centroids, revealing hidden patterns in the data.
            Source: https://serokell.io/blog/k-means-clustering-in-machine-learning
        </figcaption>
    </figure>

    <p>In the context of this project, rides can be clustered by their average and peak wait times, variability, and ride type
    to uncover categories such as “thrill rides with consistently long waits,” “family-friendly rides with steady moderate waits,”
    and “low-wait seasonal attractions.” Similarly, clustering entire days using features like crowd index, weather, and average
    waits might reveal distinct types of guest experiences, such as quiet off-season weekdays, moderate crowd days, or high-intensity
    holiday peaks. By applying clustering in these ways, this project can surface hidden patterns and relationships within the
    data that help explain how crowds and waits evolve across different rides, times, and conditions.<p>

    <figure>
        <img src="{{ url_for('static', path='figures/hierarchical_clustering_dendogram.png') }}" alt="Hierarchical clustering dendogram" width="600" style="display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center;">
            Fig. 2
        </figcaption>
        <figcaption style="text-align: center;">
            A dendrogram illustrating hierarchical clustering. Each leaf at the bottom represents an individual data point,
            and branches show how points are merged into clusters step by step. The height of each branch indicates the
            dissimilarity at which clusters were joined, with shorter branches meaning greater similarity. By choosing a
            horizontal cut across the tree, different numbers of clusters can be identified.
            Source: https://www.datanovia.com/en/lessons/examples-of-dendrograms-visualization
        </figcaption>
    </figure>

    <h3>Data Prep</h3>
    <p>Clustering is an unsupervised learning method and therefore
        requires only unlabeled numeric data. The algorithm then measures the distance between data points using metrics such
        as Euclidean distance, and groups points into clusters based on how close they are in feature space. This requirement
        for numeric data is important, because clustering relies on mathematical operations (i.e. calculating distances)
        that are only meaningful with numbers. If a dataset includes non-numeric values such as categorical labels or text,
        they must first be transformed into a numeric representation before clustering can be applied. </p>


    <figure>
      <img src="{{ url_for('static', path='figures/data_to_use_for_clustering.png') }}" alt="Clustering data" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 3. Cleaned data to be used for clustering.</figcaption>
    </figure>

    <figure>
      <img src="{{ url_for('static', path='figures/k_means_unclustered.png') }}" alt="Unclustered data" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 4. Data before clustering.</figcaption>
    </figure>

    <p><a href="https://github.com/A-Derks/CSCI_5612_project/blob/main/data/clean/queue_times_clean.csv">Here</a> is a link to the full data set that I will be using
        for clustering:</p>


    <h3>Code</h3>
    <p><a href="https://github.com/A-Derks/CSCI_5612_project/blob/main/k_means_clustering.py">Here</a> is a link to my k-means clustering code.</p>
    <p><a href="https://github.com/A-Derks/CSCI_5612_project/blob/main/hierarchical_clustering.py">Here</a> is a link to my hierarchical clustering code.</p>
    <h3>Results</h3>

    <h4>K-Means Clustering</h4>
    <figure>
      <img src="{{ url_for('static', path='figures/silhouette_method.png') }}" alt="silhouette method" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 5. Silhouette analysis to determine the optimal number of clusters (k).</figcaption>
    </figure>
    <figure>
      <img src="{{ url_for('static', path='figures/k_means_clustered_k_2.png') }}" alt="k-means clustered k = 2" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 6. K means clustering analysis with k=2.</figcaption>
    </figure>
    <figure>
      <img src="{{ url_for('static', path='figures/k_means_clustered_k_3.png') }}" alt="k-means clustered k = 3" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 7. K means clustering analysis with k=3.</figcaption>
    </figure>
    <figure>
      <img src="{{ url_for('static', path='figures/k_means_clustered_k_4.png') }}" alt="k-means clustered k = 4" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 8. K means clustering analysis with k=4.</figcaption>
    </figure>
    <figure>
      <img src="{{ url_for('static', path='figures/k_means_clustered_k_5.png') }}" alt="k-means clustered k = 5" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 9. K means clustering analysis with k=5.</figcaption>
    </figure>

    <p>Looking at the k-means results, the silhouette plot points to k = 2 as the most natural choice. In the k=2 scatter plot,
        the days fall into two clean, well-separated groups: one cluster centered around lower average and lower 95th-percentile
        waits (calmer or typical days), and a second cluster grouped at noticeably higher waits (the clearly busy days). When
        you force k=3 or k=4, the algorithm mostly chops up that lower-wait region into smaller “sub-types” of moderate days,
        but those splits overlap a lot and don’t form new, sharply separated patterns. With k=5, the clusters get even more
        fragmented—especially in the middle of the plot—so you’re adding complexity without gaining a clearer story about crowd
        behavior. Overall, k=2 keeps the interpretation simple and meaningful: Disney days in this dataset mostly behave like
        either normal/moderate days or peak-crowd days, and that two-group structure is the strongest signal in the data.</p>

    <h4>Hierarchical Clustering</h4>
    <figure>
      <img src="{{ url_for('static', path='figures/silhouette_scores.png') }}" alt="hierarchical silhouette scores" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 10. Hierarchical silhouette analysis to determine the optimal number of clusters (k).</figcaption>
    </figure>
    <figure>
      <img src="{{ url_for('static', path='figures/hierarchical_clustering_(k=3).png') }}" alt="hierarchical cluserting k=3" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 11. Hierarchical clustering with k=3 clusters.</figcaption>
    </figure>
    <figure>
      <img src="{{ url_for('static', path='figures/dendrogram.png') }}" alt="dendrogram" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 12. Hierarchical clustering dendrogram.</figcaption>
    </figure>

    <p>Using hierarchical clustering with Euclidean distance, the silhouette analysis indicates that the best separation occurs
        at k = 3, where the silhouette score reaches its maximum (about 0.61). This suggests the day-level wait-time data
        naturally breaks into three meaningful groups rather than just two. In the cluster scatter plot, these three clusters
        appear as a low-wait group (quiet days), a middle band of moderate waits, and a high-wait group (peak crowd days).</p>

    <p>The dendrogram supports this three-group structure by showing how days merge into larger branches step by step. At lower
        linkage heights, days within each tier combine tightly, while the larger jumps between branches highlight clear gaps
        between quiet, moderate, and peak days. Overall, the hierarchical results align with the k-means story of “different
        types of crowd days,” but here the tree view makes it especially clear that Disney days tend to organize into three
        distinct crowd levels, not just a simple busy/not-busy split.</p>


    <h3>Conclusions</h3>
    <p>Applying both k-means and hierarchical clustering provided consistent and meaningful insights into daily Disney park
        patterns. While the two methods use different logic, they both show that park days fall into a small number of
        repeatable crowd “types.” K-means clustering highlighted a strong split between lower-wait days and high-intensity
        crowd days, giving a clear, high-level view of quiet versus peak conditions. Hierarchical clustering refined this
        picture by separating days into three tiers—quiet, moderate, and peak—showing that there is also a distinct middle
        category of guest experiences. This outcome is valuable for the project because it demonstrates that unsupervised
        methods can uncover underlying structure in the data without prior labels. By identifying these natural day groupings,
        the analysis helps frame the broader goal of understanding crowd behavior and wait-time dynamics in theme parks.
        These results suggest that future predictive models could benefit from recognizing and adapting to these crowd tiers,
        ultimately offering more accurate and practical tools for managing expectations and planning park visits.</p>
{% endblock %}
