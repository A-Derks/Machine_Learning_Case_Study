{% extends "base.html" %}
{% block content %}
<h1>PCA</h1>
    <h3>Overview</h3>
    <p>Principal Component Analysis (PCA) is a technique used to simplify big datasets by reducing their dimensionality
        and finding the most important patterns within the data while retaining as much important information as possible. The core idea is to
        transform the original features into a new set of features, called principal components, which are uncorrelated and
        ordered (from largest to smallest) by how much information they capture from the data. This works using ideas from
        linear algebra: eigenvectors show the directions where the data spreads out the most, and eigenvalues tell us how
        much of the data’s variability each of those directions explains. The first principal component explains the most
        variation, the second explains the next most, and so on. PCA is important because highly dimensional, complex datasets
        can be messy, hard to visualize, and difficult for models to handle. By reducing dataset dimensionality, we make the data easier
        to analyze and plot, we get rid of noise or repetitive information, and we make computations faster and more efficient.</p>

    <figure>
        <img src="{{ url_for('static', path='figures/pca_overview.jpg') }}" alt="PCA overview" width="600" style="display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center;">
            Fig. 1
        </figcaption>
        <figcaption style="text-align: center;">
            An illustration of PCA: the original data (left) is rotated and projected onto new axes, PC1 and PC2,
            which capture the directions of greatest variance. The result (right) is a simplified representation
            where the main structure of the data is preserved in fewer dimensions.
            Source: https://mlpills.substack.com/p/issue-91-principal-component-analysis
        </figcaption>
    </figure>

    <figure>
        <img src="{{ url_for('static', path='figures/pca_example.png') }}" alt="PCA example" width="600" style="display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center;">
            Fig. 2
        </figcaption>
        <figcaption style="text-align: center;">
            An example PCA identifying the directions of maximum variance in the data. The new axes (principal components)
            are shown in red and green, representing the directions along which the data varies most, allowing the dataset
            to be re-expressed in terms of these components.
            Source: https://mlpills.substack.com/p/issue-91-principal-component-analysis
        </figcaption>
    </figure>

    <h3>Data Prep</h3>
    <p>All models and methods require specific data formats. PCA works only with numeric, continuous data, since it relies on
        calculating variances, covariances, and distances between data points. Because PCA is sensitive to differences in magnitude,
        each feature should ideally be on a comparable (normalized) scale (i.e. if one feature has values in the thousands and
        another is between 0 and 1, the larger feature will dominate the analysis, so we need to normalize each of those features). PCA also requires the data to be structured
        in a way where rows represent observations and columns represent features. Categorical variables can’t be used directly
        unless they are first converted into numeric form. In short, PCA needs a clean, numeric dataset, with features standardized,
        so it can correctly identify the directions of maximum variance.</p>

    <figure>
      <img src="{{ url_for('static', path='figures/pca_input_data_sample.png') }}" alt="PCA input sample" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 3. Cleaned data to be used for PCA.</figcaption>
    </figure>

    <p><a href="https://github.com/A-Derks/CSCI_5612_project/blob/main/app/static/figures/data_to_use_for_clustering.png">Here</a> is a link to the full dataset that I will be using
        for PCA.</p>

    <h3>Code</h3>
    <p><a href="https://github.com/A-Derks/CSCI_5612_project/blob/main/pca.py">Here</a> is a link to my PCA code.</p>

    <h3>Results</h3>

    <figure>
      <img src="{{ url_for('static', path='figures/pca_scatter.png') }}" alt="PCA scatter" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 4. PCA scatter plot.</figcaption>
    </figure>

    <figure>
      <img src="{{ url_for('static', path='figures/pca_scree.png') }}" alt="PCA scree" width="600" style="display:block; margin:0 auto;">
      <figcaption style="text-align:center;">Fig. 5. PCA scree plot.</figcaption>
    </figure>

    <p>The PCA results show that the Disney wait-time features can be summarized largely by a single dominant pattern.
        The scree plot indicates that PC1 alone explains about 83.5% of the variance, while PC2 adds another
        12.2%, so the first two components capture roughly 95.7% of the dataset’s structure. This means we can reduce the
        data to two dimensions with very little information loss. In the PC1 vs. PC2 scatter plot, PC1 acts as the main
        “crowd intensity” axis—days with higher overall wait levels project more strongly along this direction—while
        PC2 contributes a smaller, secondary contrast that likely reflects differences in how waits are distributed across
        rides or how open/available attractions were on a given day. Because PC3 and PC4 contribute only a few percent
        combined, additional components add minimal explanatory value, confirming that two components are sufficient for
        visualization and interpretation.</p>

    <h3>Conclusions</h3>
    <p>Overall, PCA reveals that most day-to-day variation in Disney park conditions can be traced to one primary driver
        (overall busyness) plus a much smaller secondary pattern. The fact that PC1 captures more than 80% of the
        variance shows that crowd level is the dominant force shaping wait times, and PC2 provides only a modest refinement
        to that story. For this project, that’s useful because it confirms that even though the dataset includes multiple
        metrics, the core signal is low- versus high-crowd days, with smaller differences layered on top. This dimensionality
        reduction simplifies later modeling and strengthens the takeaway that understanding and predicting crowd intensity is
        the central ingredient for explaining the Disney guest experience.</p>
{% endblock %}